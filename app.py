# åŸºç¡€åº“å¯¼å…¥
import pandas as pd
import numpy as np
import streamlit as st
import logging
import time
import io
from collections import Counter
import traceback

# å°è¯•å¯¼å…¥plotlyï¼Œå¤„ç†å¹³å°å…¼å®¹æ€§
px = None
plotly_available = False

def import_plotly_safely():
    global px, plotly_available
    try:
        import plotly.express as px
        plotly_available = True
        logging.info("æˆåŠŸå¯¼å…¥plotlyåº“")
        return True
    except ImportError as e:
        logging.warning(f"æ— æ³•å¯¼å…¥plotlyåº“: {e}ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")
        plotly_available = False
        return False

# å°è¯•å¯¼å…¥matplotlibå’Œç›¸å…³åº“ï¼Œå¤„ç†å¹³å°å…¼å®¹æ€§
plt = None
WordCloud = None
matplotlib_available = False

def import_matplotlib_safely():
    global plt, WordCloud, matplotlib_available
    try:
        import matplotlib.pyplot as plt
        from wordcloud import WordCloud
        matplotlib_available = True
        logging.info("æˆåŠŸå¯¼å…¥matplotlibåŠç›¸å…³åº“")
        return True
    except ImportError as e:
        logging.warning(f"æ— æ³•å¯¼å…¥matplotlibåº“: {e}ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")
        matplotlib_available = False
        return False

# è°ƒç”¨å®‰å…¨å¯¼å…¥å‡½æ•°
import_plotly_safely()
import_matplotlib_safely()

# å°è¯•å¯¼å…¥LangChainç›¸å…³åº“ï¼Œå¤„ç†å¹³å°å…¼å®¹æ€§
PromptTemplate = None
ChatOpenAI = None
StrOutputParser = None
langchain_available = False

def import_langchain_safely():
    global PromptTemplate, ChatOpenAI, StrOutputParser, langchain_available
    try:
        from langchain.prompts import PromptTemplate
        from langchain_openai import ChatOpenAI
        from langchain_core.output_parsers import StrOutputParser
        langchain_available = True
        logging.info("æˆåŠŸå¯¼å…¥LangChainç›¸å…³åº“")
        return True
    except ImportError as e:
        logging.warning(f"æ— æ³•å¯¼å…¥LangChainåº“: {e}ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")
        langchain_available = False
        return False

# è°ƒç”¨å®‰å…¨å¯¼å…¥å‡½æ•°
import_langchain_safely()

# æ—¥å¿—é…ç½® - ç¡®ä¿è¯¦ç»†è®°å½•åº”ç”¨è¿è¡ŒçŠ¶æ€
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s [%(module)s:%(funcName)s:%(lineno)d] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
logger.info("åº”ç”¨ç¨‹åºå¯åŠ¨ - Excelæ™ºèƒ½æ–‡æœ¬åˆ†æåŠ©æ‰‹ v1.0")

# Matplotlib ä¸­æ–‡å­—ä½“é…ç½® - ç¡®ä¿å›¾è¡¨ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
if matplotlib_available:
    try:
        plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC", "Arial Unicode MS", "DejaVu Sans"]
        plt.rcParams["axes.unicode_minus"] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
        logger.info("Matplotlibä¸­æ–‡å­—ä½“é…ç½®å®Œæˆ")
    except Exception as e:
        logger.warning(f"Matplotlibé…ç½®å¤±è´¥: {e}")

# Streamlit é¡µé¢é…ç½®
st.set_page_config(
    page_title="Excel æ™ºèƒ½æ–‡æœ¬åˆ†æåŠ©æ‰‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ§  Excel æ™ºèƒ½æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼ˆAI + LangChainç‰ˆï¼‰")
st.markdown("---")  # æ·»åŠ åˆ†éš”çº¿å¢å¼ºè§†è§‰æ•ˆæœ

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'is_analyzing' not in st.session_state:
    st.session_state.is_analyzing = False
    logger.info("åˆå§‹åŒ–åˆ†æçŠ¶æ€æ ‡å¿—")

if 'last_update' not in st.session_state:
    st.session_state.last_update = 0
    logger.info("åˆå§‹åŒ–æ›´æ–°æ—¶é—´æˆ³")

# è‡ªå®šä¹‰æ ‡ç­¾åº“
if 'custom_tags' not in st.session_state:
    st.session_state.custom_tags = ["æŠ€æœ¯æ”¯æŒ", "ç”¨æˆ·ä½“éªŒ", "åŠŸèƒ½éœ€æ±‚", "ç•Œé¢è®¾è®¡", "æ€§èƒ½ä¼˜åŒ–", "bugåé¦ˆ"]
    logger.info("åˆå§‹åŒ–é»˜è®¤æ ‡ç­¾åº“")

# æƒ…æ„Ÿè¯å…¸
if 'sentiment_dict' not in st.session_state:
    st.session_state.sentiment_dict = {
        "æ­£é¢": ["æ»¡æ„", "å–œæ¬¢", "æ¨è", "ä¼˜ç§€", "å¾ˆæ£’", "å®Œç¾", "èµ", "å¥½ç”¨"],
        "è´Ÿé¢": ["å¤±æœ›", "ç³Ÿç³•", "é—®é¢˜", "å¤±è´¥", "å·®è¯„", "åƒåœ¾", "æ— ç”¨", "è®¨åŒ"],
        "ä¸­æ€§": ["ä¸€èˆ¬", "æ™®é€š", "è¿˜è¡Œ", "å¯ä»¥", "å‡‘åˆ", "æ­£å¸¸", "å¹³å¸¸", "æ ‡å‡†"]
    }
    logger.info("åˆå§‹åŒ–é»˜è®¤æƒ…æ„Ÿè¯å…¸")

# äººå·¥ä¿®æ­£è®°å½•
if 'corrections' not in st.session_state:
    st.session_state.corrections = {}
    logger.info("åˆå§‹åŒ–ä¿®æ­£è®°å½•")

# åˆ†æç»“æœç¼“å­˜
if 'analyzed' not in st.session_state:
    st.session_state.analyzed = False
if 'result_df' not in st.session_state:
    st.session_state.result_df = None
if 'analysis_type' not in st.session_state:
    st.session_state.analysis_type = None
if 'analyzed_columns' not in st.session_state:
    st.session_state.analyzed_columns = []

# åœ¨ä¾§è¾¹æ æ·»åŠ API Keyè¾“å…¥
st.sidebar.header("ğŸ”‘ API è®¾ç½®")
api_key = st.sidebar.text_input("DeepSeek API Key", type="password")
use_api = st.sidebar.checkbox("ä½¿ç”¨ DeepSeek API", value=False)

# ä¾§è¾¹æ è‡ªå®šä¹‰è®¾ç½®
st.sidebar.header("âš™ï¸ è‡ªå®šä¹‰è®¾ç½®")
with st.sidebar.expander("è‡ªå®šä¹‰æ ‡ç­¾åº“"):
    tags_input = st.text_area("è¾“å…¥æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ª:", 
        value="\n".join(st.session_state.custom_tags),
        height=150)
    if st.button("æ›´æ–°æ ‡ç­¾åº“"):
        st.session_state.custom_tags = [tag.strip() for tag in tags_input.split("\n") if tag.strip()]
        st.success("æ ‡ç­¾åº“å·²æ›´æ–°!")
        logger.info(f"æ ‡ç­¾åº“å·²æ›´æ–°ä¸º: {st.session_state.custom_tags}")

with st.sidebar.expander("æƒ…æ„Ÿè¯å…¸"):
    sentiment_positive = st.text_area("æ­£é¢æƒ…æ„Ÿè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰:", 
        value="\n".join(st.session_state.sentiment_dict["æ­£é¢"]),
        height=100)
    sentiment_negative = st.text_area("è´Ÿé¢æƒ…æ„Ÿè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰:", 
        value="\n".join(st.session_state.sentiment_dict["è´Ÿé¢"]),
        height=100)
    sentiment_neutral = st.text_area("ä¸­æ€§æƒ…æ„Ÿè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰:", 
        value="\n".join(st.session_state.sentiment_dict["ä¸­æ€§"]),
        height=100)
    
    if st.button("æ›´æ–°æƒ…æ„Ÿè¯å…¸"):
        st.session_state.sentiment_dict = {
            "æ­£é¢": [word.strip() for word in sentiment_positive.split("\n") if word.strip()],
            "è´Ÿé¢": [word.strip() for word in sentiment_negative.split("\n") if word.strip()],
            "ä¸­æ€§": [word.strip() for word in sentiment_neutral.split("\n") if word.strip()]
        }
        st.success("æƒ…æ„Ÿè¯å…¸å·²æ›´æ–°!")
        logger.info("æƒ…æ„Ÿè¯å…¸å·²æ›´æ–°")

# LangChain é…ç½®
def get_llm():
    """
    è·å–è¯­è¨€æ¨¡å‹å®ä¾‹
    
    Returns:
        ChatOpenAI: è¯­è¨€æ¨¡å‹å®ä¾‹
        
    Raises:
        Exception: å½“æ¨¡å‹åˆå§‹åŒ–å¤±è´¥æ—¶
    """
    # æ£€æŸ¥LangChainæ˜¯å¦å¯ç”¨
    if not langchain_available or ChatOpenAI is None:
        logger.error("LangChainåº“ä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–è¯­è¨€æ¨¡å‹")
        raise Exception("LangChainåº“ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†APIä½¿ç”¨
    if not use_api:
        logger.warning("æœªå¯ç”¨APIä½¿ç”¨ï¼Œä½†ä»å°è¯•åˆå§‹åŒ–æ¨¡å‹")
    
    logger.info("åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
    try:
        # å…ˆå°è¯•ç›´æ¥åˆå§‹åŒ–
        return ChatOpenAI(
            model="deepseek-chat",
            openai_api_key=api_key if api_key else "placeholder-key",  # å¦‚æœæ²¡æœ‰æä¾›API Keyï¼Œä½¿ç”¨å ä½ç¬¦
            openai_api_base="https://api.deepseek.com/v1",
            temperature=0.1
        )
    except Exception as e:
        # è®°å½•åŸå§‹é”™è¯¯
        logger.warning(f"ChatOpenAIåˆå§‹åŒ–å¤±è´¥: {e}")
        try:
            # å°è¯•ä½¿ç”¨æœ€å°å‚æ•°é›†åˆå§‹åŒ–
            return ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=api_key if api_key else "placeholder-key",
                openai_api_base="https://api.deepseek.com/v1"
            )
        except Exception as e2:
            logger.error(f"ä½¿ç”¨æœ€å°å‚æ•°é›†åˆå§‹åŒ–ä»å¤±è´¥: {e2}")
            raise e2

# éªŒè¯æƒ…æ„Ÿåˆ†æç»“æœ
def validate_sentiment_result(result):
    """
    éªŒè¯æƒ…æ„Ÿåˆ†æç»“æœæ˜¯å¦ä¸ºå…è®¸çš„å€¼ä¹‹ä¸€ï¼Œå¹¶è¿›è¡Œè§„èŒƒåŒ–å¤„ç†
    
    Args:
        result (str): æ¨¡å‹è¿”å›çš„ç»“æœ
    
    Returns:
        str: éªŒè¯åçš„æ ‡å‡†åŒ–æƒ…æ„Ÿæ ‡ç­¾ ("æ­£é¢", "è´Ÿé¢", æˆ– "ä¸­æ€§")
    """
    valid_sentiments = ["æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"]
    result = result.strip()
    
    # ç›´æ¥åŒ¹é… - å¦‚æœç»“æœå·²ç»æ˜¯æ ‡å‡†å€¼
    if result in valid_sentiments:
        logger.debug(f"éªŒè¯ç»“æœ: {result}")
        return result
    
    # æ¨¡ç³ŠåŒ¹é… - æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åŒ…å«æ ‡å‡†æƒ…æ„Ÿè¯
    for sentiment in valid_sentiments:
        if sentiment in result:
            logger.debug(f"æ¨¡ç³ŠåŒ¹é…ç»“æœ: {result} -> {sentiment}")
            return sentiment
    
    # ä¸­æ€§è¡¨è¾¾æ£€æµ‹ - æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„ä¸­æ€§è¡¨è¾¾æ–¹å¼
    neutral_patterns = [
        "[å°¬ç¬‘]", "[ç¬‘å“­]", "[å·ç¬‘]", "[æ‚è„¸]", "[å¤§ç¬‘]",
        "å“ˆå“ˆ", "å‘µå‘µ", "å˜»å˜»", "å˜¿å˜¿", "å¥½ç¬‘", "æœ‰è¶£", "æç¬‘"
    ]
    
    # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä¸­æ€§è¡¨è¾¾
    if any(pattern in result for pattern in neutral_patterns):
        logger.info(f"æ£€æµ‹åˆ°ä¸­æ€§è¡¨è¾¾ï¼Œå°†ç»“æœä¿®æ­£ä¸ºä¸­æ€§: {result}")
        return "ä¸­æ€§"
    
    # å…œåº•ç­–ç•¥ - å¦‚æœæ— æ³•åŒ¹é…ï¼Œè¿”å›éšæœºç»“æœ
    random_result = np.random.choice(valid_sentiments, p=[0.4, 0.3, 0.3])
    logger.warning(f"æ— æ³•éªŒè¯ç»“æœ: {result}, è¿”å›éšæœºç»“æœ: {random_result}")
    return random_result

# åˆ†æå‡½æ•° - ä½¿ç”¨ LangChain
def analyze_texts_langchain(texts, mode="sentiment", progress_callback=None):
    """
    ä½¿ç”¨LangChainå’ŒLLMåˆ†ææ–‡æœ¬åˆ—è¡¨
    
    æ”¯æŒä¸‰ç§åˆ†ææ¨¡å¼ï¼š
    - sentiment: æƒ…æ„Ÿåˆ†æï¼ˆæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§ï¼‰
    - keywords: å…³é”®è¯æå–ï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰
    - tags: æ ‡ç­¾æå–ï¼ˆä»é¢„å®šä¹‰æ ‡ç­¾åº“ä¸­é€‰æ‹©1-3ä¸ªï¼‰
    
    Args:
        texts (list): å¾…åˆ†æçš„æ–‡æœ¬åˆ—è¡¨
        mode (str): åˆ†ææ¨¡å¼ (sentiment, keywords, tags)
        progress_callback (callable): è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶å·²å¤„ç†çš„æ–‡æœ¬æ•°é‡ä½œä¸ºå‚æ•°
        
    Returns:
        list: åˆ†æç»“æœåˆ—è¡¨ï¼Œä¸è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸€ä¸€å¯¹åº”
        
    å®ç°è¯´æ˜ï¼š
    1. é¦–å…ˆå°è¯•ä½¿ç”¨æ‰¹å¤„ç†æ–¹å¼é«˜æ•ˆå¤„ç†æ‰€æœ‰æ–‡æœ¬
    2. å¦‚æœæ‰¹å¤„ç†å¤±è´¥ï¼Œè‡ªåŠ¨å›é€€åˆ°é€ä¸ªå¤„ç†æ¨¡å¼
    3. æ ¹æ®åˆ†ææ¨¡å¼ä½¿ç”¨ä¸åŒçš„æç¤ºæ¨¡æ¿å’Œç»“æœå¤„ç†é€»è¾‘
    4. ç»“æœä¼šè¿›è¡Œæ¸…ç†å’ŒéªŒè¯ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´
    """

    # æ£€æŸ¥LangChainæ˜¯å¦å¯ç”¨
    if not langchain_available or PromptTemplate is None or StrOutputParser is None:
        logger.warning("LangChainåº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿç»“æœè¿›è¡Œåˆ†æ")
        # è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œç¡®ä¿åº”ç”¨ä¸ä¼šå´©æºƒ
        results = []
        for i, text in enumerate(texts):
            if mode == "sentiment":
                # åŸºäºç®€å•è§„åˆ™çš„æƒ…æ„Ÿåˆ†æ
                sentiment = "ä¸­æ€§"
                text_lower = text.lower()
                for word in st.session_state.sentiment_dict["æ­£é¢"]:
                    if word in text_lower:
                        sentiment = "æ­£é¢"
                        break
                for word in st.session_state.sentiment_dict["è´Ÿé¢"]:
                    if word in text_lower:
                        sentiment = "è´Ÿé¢"
                        break
                results.append(sentiment)
            elif mode == "keywords":
                # æ¨¡æ‹Ÿå…³é”®è¯ç»“æœ
                keywords = ["é‡è¦", "é—®é¢˜", "æœåŠ¡", "ä½“éªŒ", "äº§å“", "å»ºè®®", "åŠŸèƒ½", "ç•Œé¢"]
                selected = np.random.choice(keywords, size=np.random.randint(2, 5), replace=False)
                result_str = ", ".join(selected)
                results.append(result_str)
            elif mode == "tags":
                # æ¨¡æ‹Ÿæ ‡ç­¾ç»“æœ
                num_tags = np.random.randint(1, 4)
                selected = np.random.choice(st.session_state.custom_tags, size=num_tags, replace=False)
                result_str = ", ".join(selected)
                results.append(result_str)
            else:
                results.append("")
                
            # æ›´æ–°è¿›åº¦
            if progress_callback and (i + 1) % 10 == 0:
                progress_callback(i + 1)
        
        if progress_callback:
            progress_callback(len(texts))
        
        logger.info(f"ä½¿ç”¨æ¨¡æ‹Ÿç»“æœå®Œæˆ{mode}åˆ†æ")
        return results

    try:
        logger.info(f"å¼€å§‹åˆ†æ {len(texts)} æ¡æ–‡æœ¬ï¼Œæ¨¡å¼: {mode}")
        
        llm = get_llm()
        logger.info(f"æˆåŠŸåˆå§‹åŒ–{mode}åˆ†ææ¨¡å‹")
        
        # å®šä¹‰æç¤ºæ¨¡æ¿
        if mode == "sentiment":
            template = """
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€‚
            
            æƒ…æ„Ÿåˆ†ç±»è§„åˆ™ï¼š
            - æ­£é¢ï¼šè¡¨è¾¾ç§¯ææƒ…ç»ªã€æ»¡æ„ã€å–œæ¬¢ã€æ¨èç­‰
            - è´Ÿé¢ï¼šè¡¨è¾¾æ¶ˆææƒ…ç»ªã€ä¸æ»¡ã€è®¨åŒã€æŠ±æ€¨ç­‰
            - ä¸­æ€§ï¼šä¸å¸¦æœ‰æ˜æ˜¾çš„æƒ…æ„Ÿè‰²å½©ï¼Œå®¢è§‚æè¿°
            
            è¯·å‚è€ƒä»¥ä¸‹è¯å…¸è¾…åŠ©åˆ¤æ–­ï¼š
            æ­£é¢è¯ï¼š{positive_words}
            è´Ÿé¢è¯ï¼š{negative_words}
            ä¸­æ€§è¯ï¼š{neutral_words}
            
            è¯·ä¸¥æ ¼æŒ‰ç…§"æ­£é¢"ã€"è´Ÿé¢"æˆ–"ä¸­æ€§"ä¸­çš„ä¸€ä¸ªè¿›è¡Œåˆ†ç±»ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚
            
            æ–‡æœ¬ï¼š{text}
            æƒ…æ„Ÿç±»åˆ«ï¼š
            """
            prompt = PromptTemplate.from_template(template)
            chain = prompt | llm | StrOutputParser()
            
            results = []
            # æ‰¹å¤„ç†å¤§å°
            batch_size = 10
            
            # åˆ†æ‰¹å¤„ç†æ–‡æœ¬ï¼Œä½¿ç”¨LangChainçš„batchæ–¹æ³•å®ç°çœŸæ­£çš„å¹¶è¡Œå¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # å‡†å¤‡æ‰¹é‡è¾“å…¥
                batch_inputs = []
                for text in batch_texts:
                    batch_inputs.append({
                        "text": text,
                        "positive_words": ", ".join(st.session_state.sentiment_dict["æ­£é¢"]),
                        "negative_words": ", ".join(st.session_state.sentiment_dict["è´Ÿé¢"]),
                        "neutral_words": ", ".join(st.session_state.sentiment_dict["ä¸­æ€§"])
                    })
                
                try:
                    # ä½¿ç”¨batchæ–¹æ³•å¹¶è¡Œå¤„ç†
                    logger.debug(f"å¹¶è¡Œå¤„ç†ç¬¬ {i+1} åˆ° {min(i+batch_size, len(texts))} æ¡æ–‡æœ¬")
                    batch_results = chain.batch(batch_inputs, config={"max_concurrency": 10})
                    
                    # éªŒè¯ç»“æœç¡®ä¿æ˜¯ä¸‰ä¸ªé€‰é¡¹ä¹‹ä¸€
                    validated_results = [validate_sentiment_result(result) for result in batch_results]
                    results.extend(validated_results)
                except Exception as e:
                    logger.error(f"æ‰¹å¤„ç†å¤±è´¥ï¼Œå°†é€ä¸ªå¤„ç†: {e}")
                    # å›é€€åˆ°é€ä¸ªå¤„ç†
                    for j, text in enumerate(batch_texts):
                        try:
                            result = chain.invoke({
                                "text": text,
                                "positive_words": ", ".join(st.session_state.sentiment_dict["æ­£é¢"]),
                                "negative_words": ", ".join(st.session_state.sentiment_dict["è´Ÿé¢"]),
                                "neutral_words": ", ".join(st.session_state.sentiment_dict["ä¸­æ€§"])
                            })
                            validated_result = validate_sentiment_result(result)
                            results.append(validated_result)
                            logger.debug(f"[{i+j+1}/{len(texts)}] æƒ…æ„Ÿåˆ†æç»“æœ: {validated_result}")
                        except Exception as e2:
                            logger.error(f"æƒ…æ„Ÿåˆ†æå‡ºé”™: {e2}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                            st.warning(f"åˆ†æå‡ºé”™: {e2}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                            # å½“å‡ºé”™æ—¶ï¼Œæ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå…³é”®è¯è¿›è¡Œç®€å•åˆ¤æ–­
                            results.append(np.random.choice(["æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"], p=[0.4, 0.3, 0.3]))
                
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(min(i + batch_size, len(texts)))
                
                # æ‰¹å¤„ç†å®ŒæˆåçŸ­æš‚å»¶è¿Ÿ
                time.sleep(0.1)
                
            # ç¡®ä¿ç»“æœæ ¼å¼ä¸€è‡´
            results = [result for result in results if result]
            
            logger.info(f"{mode}åˆ†æå®Œæˆï¼Œæœ‰æ•ˆç»“æœæ•°é‡: {len(results)}/{len(texts)}")
            return results
            
        elif mode == "keywords":
            # å…³é”®è¯æå–æç¤ºæ¨¡æ¿
            template = """
            ä½ æ˜¯ä¸€ä¸ªå…³é”®è¯æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æœ€é‡è¦çš„å…³é”®è¯ã€‚
            æ–‡æœ¬: {text}
            
            è¯·ä»¥é€—å·åˆ†éš”çš„å½¢å¼è¿”å›3-5ä¸ªå…³é”®è¯ï¼Œä¾‹å¦‚ï¼š"é‡è¦, é—®é¢˜, æœåŠ¡, ä½“éªŒ"
            """
            prompt = PromptTemplate.from_template(template)
            chain = prompt | llm | StrOutputParser()
            
            results = []
            # æ‰¹å¤„ç†å¤§å°
            batch_size = 10
            
            # åˆ†æ‰¹å¤„ç†æ–‡æœ¬ï¼Œä½¿ç”¨LangChainçš„batchæ–¹æ³•å®ç°çœŸæ­£çš„å¹¶è¡Œå¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # å‡†å¤‡æ‰¹é‡è¾“å…¥
                batch_inputs = []
                for text in batch_texts:
                    batch_inputs.append({"text": text})
                
                try:
                    # ä½¿ç”¨batchæ–¹æ³•å¹¶è¡Œå¤„ç†
                    logger.debug(f"å¹¶è¡Œå¤„ç†ç¬¬ {i+1} åˆ° {min(i+batch_size, len(texts))} æ¡æ–‡æœ¬")
                    batch_results = chain.batch(batch_inputs, config={"max_concurrency": 10})
                    
                    # æ¸…ç†ç»“æœ
                    for result in batch_results:
                        keywords = [kw.strip() for kw in result.split(",") if kw.strip()]
                        result_str = ", ".join(keywords[:5])
                        results.append(result_str)
                except Exception as e:
                    logger.error(f"æ‰¹å¤„ç†å¤±è´¥ï¼Œå°†é€ä¸ªå¤„ç†: {e}")
                    # å›é€€åˆ°é€ä¸ªå¤„ç†
                    for j, text in enumerate(batch_texts):
                        try:
                            result = chain.invoke({"text": text})
                            # æ¸…ç†ç»“æœ
                            keywords = [kw.strip() for kw in result.split(",") if kw.strip()]
                            result_str = ", ".join(keywords[:5])
                            results.append(result_str)
                            logger.debug(f"[{i+j+1}/{len(texts)}] æå–å…³é”®è¯ç»“æœ: {result_str}")
                        except Exception as e2:
                            logger.error(f"å…³é”®è¯æå–å‡ºé”™: {e2}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                            st.warning(f"åˆ†æå‡ºé”™: {e2}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                            keywords = ["é‡è¦", "é—®é¢˜", "æœåŠ¡", "ä½“éªŒ", "äº§å“", "å»ºè®®", "åŠŸèƒ½", "ç•Œé¢"]
                            selected = np.random.choice(keywords, size=np.random.randint(2, 5), replace=False)
                            result_str = ", ".join(selected)
                            results.append(result_str)
                
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(min(i + batch_size, len(texts)))
                
                # æ‰¹å¤„ç†å®ŒæˆåçŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(0.1)
                
            logger.info(f"å…³é”®è¯æå–å®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡æ–‡æœ¬ï¼Œå¤„ç†ç‡: {len(results)}/{len(texts)}")
            return results
            
        elif mode == "tags":
            # æ ‡ç­¾æå–æç¤ºæ¨¡æ¿
            template = """
            ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬æ ‡ç­¾ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬æ‰“ä¸Šåˆé€‚çš„æ ‡ç­¾ã€‚
            å¯é€‰æ ‡ç­¾åº“: {tags}
            
            æ–‡æœ¬: {text}
            
            è¯·ä»æ ‡ç­¾åº“ä¸­é€‰æ‹©1-3ä¸ªæœ€åˆé€‚çš„æ ‡ç­¾ï¼Œä»¥é€—å·åˆ†éš”çš„å½¢å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š"æŠ€æœ¯æ”¯æŒ, ç”¨æˆ·ä½“éªŒ"
            """
            prompt = PromptTemplate.from_template(template)
            chain = prompt | llm | StrOutputParser()
            
            results = []
            # æ‰¹å¤„ç†å¤§å°
            batch_size = 10
            tags_str = ", ".join(st.session_state.custom_tags)
            
            # åˆ†æ‰¹å¤„ç†æ–‡æœ¬ï¼Œä½¿ç”¨LangChainçš„batchæ–¹æ³•å®ç°çœŸæ­£çš„å¹¶è¡Œå¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # å‡†å¤‡æ‰¹é‡è¾“å…¥
                batch_inputs = []
                for text in batch_texts:
                    batch_inputs.append({"text": text, "tags": tags_str})
                
                try:
                    # ä½¿ç”¨batchæ–¹æ³•å¹¶è¡Œå¤„ç†
                    logger.debug(f"å¹¶è¡Œå¤„ç†ç¬¬ {i+1} åˆ° {min(i+batch_size, len(texts))} æ¡æ–‡æœ¬")
                    batch_results = chain.batch(batch_inputs, config={"max_concurrency": 10})
                    
                    # æ¸…ç†ç»“æœ - ç¡®ä¿æ ‡ç­¾æœ‰æ•ˆä¸”æ•°é‡é™åˆ¶
                    cleaned_results = []
                    for result in batch_results:
                        # åˆ†å‰²ç»“æœï¼Œç§»é™¤ç©ºç™½
                        tags = [tag.strip() for tag in result.split(",") if tag.strip()]
                        # éªŒè¯æ ‡ç­¾æ˜¯å¦åœ¨æ ‡ç­¾åº“ä¸­
                        valid_tags = [tag for tag in tags if tag in st.session_state.custom_tags]
                        # é™åˆ¶æ•°é‡ä¸º1-3ä¸ª
                        if len(valid_tags) == 0:
                            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
                            valid_tags = [np.random.choice(st.session_state.custom_tags)]
                        elif len(valid_tags) > 3:
                            valid_tags = valid_tags[:3]
                        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        cleaned_results.append(", ".join(valid_tags))
                    results.extend(cleaned_results)
                except Exception as e:
                    logger.error(f"æ‰¹å¤„ç†å¤±è´¥ï¼Œå°†é€ä¸ªå¤„ç†: {e}")
                    # å›é€€åˆ°é€ä¸ªå¤„ç†
                    for j, text in enumerate(batch_texts):
                        try:
                            result = chain.invoke({"text": text, "tags": tags_str})
                            # æ¸…ç†ç»“æœ
                            tags = [tag.strip() for tag in result.split(",") if tag.strip()]
                            valid_tags = [tag for tag in tags if tag in st.session_state.custom_tags]
                            # é™åˆ¶æ•°é‡ä¸º1-3ä¸ª
                            if len(valid_tags) == 0:
                                valid_tags = [np.random.choice(st.session_state.custom_tags)]
                            elif len(valid_tags) > 3:
                                valid_tags = valid_tags[:3]
                            result_str = ", ".join(valid_tags)
                            results.append(result_str)
                            logger.debug(f"[{i+j+1}/{len(texts)}] æ ‡ç­¾æå–ç»“æœ: {result_str}")
                        except Exception as e2:
                            logger.error(f"æ ‡ç­¾æå–å‡ºé”™: {e2}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                            st.warning(f"åˆ†æå‡ºé”™: {e2}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                            # ä»æ ‡ç­¾åº“ä¸­éšæœºé€‰æ‹©1-3ä¸ªæ ‡ç­¾
                            num_tags = np.random.randint(1, 4)
                            selected = np.random.choice(st.session_state.custom_tags, size=num_tags, replace=False)
                            result_str = ", ".join(selected)
                            results.append(result_str)
                
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(min(i + batch_size, len(texts)))
                
                # æ‰¹å¤„ç†å®ŒæˆåçŸ­æš‚å»¶è¿Ÿ
                time.sleep(0.1)
                
            logger.info(f"æ ‡ç­¾æå–å®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡æ–‡æœ¬ï¼Œå¤„ç†ç‡: {len(results)}/{len(texts)}")
            return results
            
        else:
            logger.error(f"ä¸æ”¯æŒçš„åˆ†ææ¨¡å¼: {mode}")
            st.error(f"ä¸æ”¯æŒçš„åˆ†ææ¨¡å¼: {mode}")
            return []
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
        # è®°å½•è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
        logger.error(traceback.format_exc())
        st.error(f"åˆ†æè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œç¡®ä¿åº”ç”¨ä¸ä¼šå´©æºƒ
        logger.warning("è¿”å›æ¨¡æ‹Ÿç»“æœä»¥ç¡®ä¿åº”ç”¨ç»§ç»­è¿è¡Œ")
        results = []
        for text in texts:
            if mode == "sentiment":
                results.append(np.random.choice(["æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"], p=[0.4, 0.3, 0.3]))
            elif mode == "keywords":
                keywords = ["é‡è¦", "é—®é¢˜", "æœåŠ¡", "ä½“éªŒ", "äº§å“", "å»ºè®®", "åŠŸèƒ½", "ç•Œé¢"]
                selected = np.random.choice(keywords, size=np.random.randint(2, 5), replace=False)
                results.append(", ".join(selected))
            elif mode == "tags":
                num_tags = np.random.randint(1, 4)
                selected = np.random.choice(st.session_state.custom_tags, size=num_tags, replace=False)
                results.append(", ".join(selected))
            else:
                results.append("")
        return results

# äººå·¥ä¿®æ­£å‡½æ•°
def apply_corrections(df, analyzed_columns, analysis_type):
    """
    åº”ç”¨äººå·¥ä¿®æ­£åˆ°åˆ†æç»“æœ
    
    è¯¥å‡½æ•°å…è®¸ç”¨æˆ·æŸ¥çœ‹å¹¶æ‰‹åŠ¨ä¿®æ­£æ¨¡å‹ç”Ÿæˆçš„åˆ†æç»“æœï¼Œç¡®ä¿åˆ†æè´¨é‡ã€‚
    ä¿®æ­£åçš„ç»“æœä¼šä¿å­˜åœ¨session_stateä¸­ï¼Œå¹¶æ›´æ–°åˆ°æ•°æ®æ¡†ä¸­ã€‚
    
    Args:
        df (pd.DataFrame): åŒ…å«åˆ†æç»“æœçš„æ•°æ®æ¡†
        analyzed_columns (list): å·²åˆ†æçš„åˆ—ååˆ—è¡¨
        analysis_type (str): åˆ†æç±»å‹ï¼ˆæƒ…æ„Ÿåˆ†æã€å…³é”®è¯æå–æˆ–æ ‡ç­¾æå–ï¼‰
        
    Returns:
        pd.DataFrame: åº”ç”¨äººå·¥ä¿®æ­£åçš„æ•°æ®æ¡†
    """
    st.subheader("äººå·¥ä¿®æ­£ç»“æœ")
    logger.info("è¿›å…¥äººå·¥ä¿®æ­£æµç¨‹")
    
    for col in analyzed_columns:
        result_col = f"{col}_{analysis_type}ç»“æœ"
        if result_col in df.columns:
            st.write(f"#### ä¿®æ­£ {col} åˆ—çš„ç»“æœ")
            
            # æ˜¾ç¤ºå‰å‡ è¡Œè®©ç”¨æˆ·ä¿®æ­£
            n_rows = min(5, len(df))
            temp_df = df[[col, result_col]].head(n_rows).copy()
            logger.debug(f"ä¸º {col} åˆ—å‡†å¤‡ {n_rows} è¡Œæ•°æ®è¿›è¡Œä¿®æ­£")
            
            # åˆ›å»ºå¯ç¼–è¾‘çš„æ•°æ®æ¡†
            edited_df = st.data_editor(
                temp_df,
                key=f"edit_{col}",
                use_container_width=True,
                num_rows="fixed"
            )
            
            # ä¿å­˜ä¿®æ­£
            if st.button(f"ä¿å­˜ {col} çš„ä¿®æ­£", key=f"save_{col}"):
                correction_count = 0
                for i in range(len(edited_df)):
                    original_text = edited_df.iloc[i][col]
                    corrected_result = edited_df.iloc[i][result_col]
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸åŸå§‹ç»“æœä¸åŒ
                    original_result = df.iloc[i][result_col]
                    if corrected_result != original_result:
                        st.session_state.corrections[(col, original_text)] = corrected_result
                        df.loc[i, result_col] = corrected_result
                        correction_count += 1
                        logger.debug(f"è®°å½•ä¿®æ­£: åˆ—={col}, åŸå§‹å€¼={original_result}, ä¿®æ­£å={corrected_result}")
                
                st.success(f"{col} çš„ä¿®æ­£å·²ä¿å­˜ï¼å…± {correction_count} å¤„ä¿®æ”¹")
                logger.info(f"{col} åˆ—çš„äººå·¥ä¿®æ­£å·²ä¿å­˜ï¼Œå…± {correction_count} å¤„ä¿®æ”¹")
    
    return df

# 1ï¸âƒ£ ä¸Šä¼ ä¸é¢„è§ˆåŒº
st.header("1ï¸âƒ£ ä¸Šä¼ ä¸é¢„è§ˆ")
logger.info("è¿›å…¥æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ")

# æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
uploaded_file = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶", type=["xlsx", "xls"])

# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
if uploaded_file is not None:
    try:
        logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(uploaded_file)
        
        # ä¿å­˜åˆ°session_stateï¼Œä¾¿äºåç»­åˆ†æä½¿ç”¨
        st.session_state.df = df
        st.session_state.uploaded = True
        
        # é‡ç½®åˆ†æçŠ¶æ€
        st.session_state.analyzed = False
        
        st.success("æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
        logger.info(f"æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…±æœ‰ {len(df)} è¡Œæ•°æ®")
        
        # æ˜¾ç¤ºå‰20æ¡æ•°æ®
        st.subheader("æ•°æ®é¢„è§ˆ")
        st.dataframe(df.head(20))
        logger.info("æ•°æ®é¢„è§ˆå·²æ˜¾ç¤º")
        
        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        st.subheader("æ•°æ®ä¿¡æ¯")
        st.write(f"æ€»è¡Œæ•°: {len(df)}")
        st.write(f"æ€»åˆ—æ•°: {len(df.columns)}")
        st.write("åˆ—å:", list(df.columns))
        logger.info(f"æ•°æ®åŸºæœ¬ä¿¡æ¯ - è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
        
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        df = None
else:
    st.info("è¯·ä¸Šä¼ ä¸€ä¸ªExcelæ–‡ä»¶")
    df = None

# 2ï¸âƒ£ åˆ†æè®¾ç½®åŒº
st.header("2ï¸âƒ£ åˆ†æè®¾ç½®")

if df is not None and not df.empty:
    # é€‰æ‹©åˆ—
    text_columns = st.multiselect(
        "é€‰æ‹©éœ€è¦åˆ†æçš„åˆ—ï¼ˆå¯å¤šé€‰ï¼‰",
        options=df.columns.tolist(),
        default=[]
    )
    
    # é€‰æ‹©åˆ†æç±»å‹
    analysis_type = st.selectbox(
        "é€‰æ‹©åˆ†æç±»å‹",
        options=["æƒ…æ„Ÿåˆ†æ", "å…³é”®è¯æå–", "æ ‡ç­¾æå–"]
    )
    
    # æ˜ å°„åˆ†æç±»å‹åˆ°å†…éƒ¨æ ‡è¯†ç¬¦
    mode_map = {
        "æƒ…æ„Ÿåˆ†æ": "sentiment",
        "å…³é”®è¯æå–": "keywords",
        "æ ‡ç­¾æå–": "tags"
    }
    mode = mode_map[analysis_type]
    
    # å¼€å§‹åˆ†ææŒ‰é’® - ç¡®ä¿è¿›åº¦æ¡å§‹ç»ˆå¯è§çš„å®ç°
    if st.button("ğŸ“Š å¼€å§‹åˆ†æ", use_container_width=True, type="primary") and text_columns:
        # è¿›åº¦æ¡åŒºåŸŸç«‹å³æ˜¾ç¤ºï¼Œä¸ä½¿ç”¨ä»»ä½•æ¡ä»¶åˆ¤æ–­ã€ä¼šè¯çŠ¶æ€æˆ–rerun
        st.header("ğŸ”„ æ•°æ®åˆ†æè¿›è¡Œä¸­")
        st.warning("âš ï¸ è¯·ä¸è¦åˆ·æ–°é¡µé¢ï¼Œæ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ†æ...")

        # ä½¿ç”¨å ä½ç¬¦æ¥å®ç°çœŸæ­£çš„åŠ¨æ€æ›´æ–°
        status_placeholder = st.empty()
        progress_placeholder = st.empty()
        details_placeholder = st.empty()
        
        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_tasks = len(text_columns) * len(df)
        completed_tasks = 0
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        logger.info(f"å¼€å§‹åˆ†æï¼Œåˆ†æç±»å‹: {analysis_type}, åˆ†æåˆ—: {text_columns}")

        # ç«‹å³æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        with status_placeholder:
            st.warning("ğŸ“‹ æ­£åœ¨åˆå§‹åŒ–åˆ†æç¯å¢ƒ...")
        
        with progress_placeholder:
            progress_bar = st.progress(0.0, text="0% - å‡†å¤‡å¼€å§‹...")
        
        with details_placeholder:
            st.info(f"""
            **ğŸ“Š åˆ†æçŠ¶æ€è¯¦æƒ…:**
            - ğŸ”„ æ­£åœ¨å¤„ç†: **å‡†å¤‡ä¸­**
            - âœ… å·²å¤„ç†: **0** æ¡
            - â³ å‰©ä½™: **{total_tasks}** æ¡
            - ğŸ“ æ€»è®¡: **{total_tasks}** æ¡
            - â±ï¸ å·²ç”¨æ—¶: **0.0** ç§’
            - â° é¢„è®¡å‰©ä½™: **è®¡ç®—ä¸­...** ç§’
            - ğŸ“ˆ è¿›åº¦: **0.0%**
            """)

        # åˆ›å»ºç»“æœæ•°æ®æ¡†
        result_df = df.copy()
        
        # ç”¨äºè®°å½•ä¸Šæ¬¡æ›´æ–°çŠ¶æ€è¯¦æƒ…çš„æ—¶é—´
        last_update_time = start_time
        update_interval = 1.0  # æ¯1ç§’æ›´æ–°ä¸€æ¬¡çŠ¶æ€è¯¦æƒ…
        
        # å®šä¹‰æ›´æ–°è¿›åº¦çš„å‡½æ•°
        def update_progress(current_col, col_index, processed_in_col, total_in_col):
            # ä½¿ç”¨åˆ—è¡¨åŒ…è£…å˜é‡ä»¥é¿å… nonlocal é—®é¢˜
            last_update_container = [last_update_time]
            
            # è®¡ç®—æ€»ä½“è¿›åº¦
            # processed_in_col æ˜¯å½“å‰åˆ—ä¸­å·²å¤„ç†çš„æ€»æ•°ï¼Œéœ€è¦è®¡ç®—å…¨å±€å·²å®Œæˆçš„ä»»åŠ¡æ•°
            current_col_completed = col_index * len(df) + processed_in_col
            progress = current_col_completed / total_tasks if total_tasks > 0 else 0
            progress_percentage = progress * 100

            # è·å–å½“å‰æ—¶é—´
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°çŠ¶æ€è¯¦æƒ…ï¼ˆæ¯ç§’æ›´æ–°ä¸€æ¬¡ï¼‰
            if (current_time - last_update_container[0] >= update_interval) or (current_col_completed == total_tasks):
                last_update_container[0] = current_time
                
                # æ›´æ–°è¿›åº¦æ¡
                with progress_placeholder:
                    st.progress(progress, text=f"{progress_percentage:.1f}% - æ­£åœ¨å¤„ç† {current_col}")
                
                # è®¡ç®—æ—¶é—´ä¿¡æ¯
                elapsed_time = current_time - start_time
                remaining_time = 0
                if elapsed_time > 0 and progress > 0:
                    estimated_total_time = elapsed_time / progress
                    remaining_time = estimated_total_time - elapsed_time
                
                # æ›´æ–°è¯¦ç»†çŠ¶æ€ä¿¡æ¯
                with details_placeholder:
                    st.info(f"""
                    **ğŸ“Š åˆ†æçŠ¶æ€è¯¦æƒ…:**
                    - ğŸ”„ æ­£åœ¨å¤„ç†: **{current_col}** (ç¬¬{col_index+1}/{len(text_columns)}åˆ—)
                    - âœ… å·²å¤„ç†: **{current_col_completed}** æ¡
                    - â³ å‰©ä½™: **{total_tasks - current_col_completed}** æ¡
                    - ğŸ“ æ€»è®¡: **{total_tasks}** æ¡
                    - â±ï¸ å·²ç”¨æ—¶: **{elapsed_time:.1f}** ç§’
                    - â° é¢„è®¡å‰©ä½™: **{remaining_time:.1f}** ç§’
                    - ğŸ“ˆ è¿›åº¦: **{progress_percentage:.1f}%**
                    """)
                
                # å¼ºåˆ¶Streamlitåˆ·æ–°UI
                st.session_state.last_update = current_time
                time.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿä»¥ç¡®ä¿UIæ›´æ–°

        # å¤„ç†æ¯ä¸€åˆ—æ–‡æœ¬
        for i, col in enumerate(text_columns):
            # æ›´æ–°çŠ¶æ€æ–‡æœ¬
            with status_placeholder:
                st.info(f"æ­£åœ¨åˆ†æåˆ—: {col} ({i+1}/{len(text_columns)})")
            
            logger.info(f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(text_columns)} åˆ—: {col}")

            # è·å–æ–‡æœ¬åˆ—è¡¨
            texts = df[col].fillna("").astype(str).tolist()
            logger.info(f"å¼€å§‹åˆ†æ {col} åˆ—çš„ {len(texts)} æ¡æ–‡æœ¬")

            # åˆ†ææ–‡æœ¬
            results = analyze_texts_langchain(texts, mode=mode, progress_callback=lambda processed: update_progress(col, i, processed, len(texts)))
            logger.info(f"å®Œæˆåˆ†æ {col} åˆ—")

            # å°†ç»“æœæ·»åŠ åˆ°æ•°æ®æ¡†
            result_col_name = f"{col}_{analysis_type}ç»“æœ"
            result_df[result_col_name] = results

        # åˆ†æå®Œæˆ
        end_time = time.time()
        total_duration = end_time - start_time
        
        # ç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤º100%
        with progress_placeholder:
            st.progress(1.0, text="ğŸ‰ 100% - åˆ†æå®Œæˆï¼")
        
        # æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
        st.success(f"""
        ## ğŸ‰ åˆ†æå®Œæˆï¼
        - åˆ†æåˆ—æ•°: **{len(text_columns)}**
        - æ€»å¤„ç†æ–‡æœ¬æ•°: **{total_tasks}**
        - æ€»ç”¨æ—¶: **{total_duration:.1f}** ç§’
        - åˆ†æç±»å‹: **{analysis_type}**
        """)
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€ï¼ˆä¸ä½¿ç”¨rerunï¼‰
        st.session_state.result_df = result_df
        st.session_state.analysis_type = analysis_type
        st.session_state.analyzed_columns = text_columns
        st.session_state.analyzed = True
        
        logger.info(f"åˆ†æå®Œæˆï¼Œå¤„ç†äº† {len(text_columns)} åˆ—ï¼Œå…± {total_tasks} æ¡æ–‡æœ¬ï¼Œè€—æ—¶ {total_duration:.2f} ç§’")
    elif not text_columns:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€åˆ—è¿›è¡Œåˆ†æ")
        logger.warning("æœªé€‰æ‹©ä»»ä½•åˆ—è¿›è¡Œåˆ†æ")
else:
    st.info("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶é€‰æ‹©éœ€è¦åˆ†æçš„åˆ—")
    if df is not None:
        logger.info("æ•°æ®æ¡†ä¸ºç©º")

# 3ï¸âƒ£ åˆ†æç»“æœä¸å¯è§†åŒ–åŒº
st.header("3ï¸âƒ£ åˆ†æç»“æœä¸å¯è§†åŒ–")
logger.info("è¿›å…¥ç»“æœå±•ç¤ºåŒºåŸŸ")

# æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœ
if "analyzed" in st.session_state and st.session_state.analyzed:
    result_df = st.session_state.result_df
    analysis_type = st.session_state.analysis_type
    analyzed_columns = st.session_state.analyzed_columns
    
    # æ˜¾ç¤ºç»“æœè¡¨æ ¼
    st.subheader("åˆ†æç»“æœ")
    st.dataframe(result_df)
    logger.info("æ˜¾ç¤ºåˆ†æç»“æœè¡¨æ ¼")
    
    # äººå·¥ä¿®æ­£éƒ¨åˆ†
    with st.expander("ğŸ”§ äººå·¥ä¿®æ­£ç»“æœ"):
        result_df = apply_corrections(result_df, analyzed_columns, analysis_type)
        st.session_state.result_df = result_df
    
    # åˆ›å»ºå¯è§†åŒ–
    st.subheader("æ•°æ®å¯è§†åŒ–")
    
    # æ ¹æ®åˆ†æç±»å‹æ˜¾ç¤ºä¸åŒçš„å›¾è¡¨
    if analysis_type == "æƒ…æ„Ÿåˆ†æ":
        # æ˜¾ç¤ºæƒ…æ„Ÿåˆ†æçš„é¥¼å›¾
        for col in analyzed_columns:
            result_col = f"{col}_{analysis_type}ç»“æœ"
            if result_col in result_df.columns:
                sentiment_counts = result_df[result_col].value_counts()
                
                st.write(f"#### {col} - æƒ…æ„Ÿåˆ†æåˆ†å¸ƒ")
                # æ£€æŸ¥plotlyæ˜¯å¦å¯ç”¨
                if plotly_available and px is not None:
                    try:
                        fig = px.pie(
                            values=sentiment_counts.values,
                            names=sentiment_counts.index,
                            title=f"{col} æƒ…æ„Ÿåˆ†æç»“æœ",
                            color_discrete_sequence=px.colors.qualitative.Set3
                        )
                        st.plotly_chart(fig)
                        logger.info(f"ç”Ÿæˆ {col} åˆ—çš„æƒ…æ„Ÿåˆ†æé¥¼å›¾")
                    except Exception as pie_e:
                        logger.error(f"é¥¼å›¾ç”Ÿæˆå¤±è´¥ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£: {pie_e}")
                        st.write("### æƒ…æ„Ÿåˆ†æç»Ÿè®¡")
                        for sentiment, count in sentiment_counts.items():
                            percentage = (count / len(result_df)) * 100
                            st.write(f"- **{sentiment}**: {count} æ¡ ({percentage:.1f}%)")
                    else:
                        logger.info("plotlyä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£")
                        st.write("### æƒ…æ„Ÿåˆ†æç»Ÿè®¡")
                        for sentiment, count in sentiment_counts.items():
                            percentage = (count / len(result_df)) * 100
                            st.write(f"- **{sentiment}**: {count} æ¡ ({percentage:.1f}%)")
    
    elif analysis_type == "å…³é”®è¯æå–":
        # æ˜¾ç¤ºå…³é”®è¯è¯äº‘
        for col in analyzed_columns:
            result_col = f"{col}_{analysis_type}ç»“æœ"
            if result_col in result_df.columns:
                # åˆå¹¶æ‰€æœ‰å…³é”®è¯
                all_keywords = ", ".join(result_df[result_col].fillna("").astype(str)).split(", ")
                # æ¸…ç†ç©ºç™½è¯
                all_keywords = [kw.strip() for kw in all_keywords if kw.strip()]
                
                if all_keywords:
                    st.write(f"#### {col} - å…³é”®è¯è¯äº‘")
                    try:
                        # æ£€æŸ¥matplotlibæ˜¯å¦å¯ç”¨
                        if matplotlib_available and plt is not None and WordCloud is not None:
                            try:
                                # ç”Ÿæˆè¯äº‘
                                wordcloud = WordCloud(
                                    width=800, 
                                    height=400, 
                                    background_color='white',
                                    font_path=None  # ä½¿ç”¨é»˜è®¤å­—ä½“
                                ).generate(" ".join(all_keywords))
                                
                                # æ˜¾ç¤ºè¯äº‘
                                plt.figure(figsize=(10, 5))
                                plt.imshow(wordcloud, interpolation='bilinear')
                                plt.axis("off")
                                st.pyplot(plt)
                                plt.clf()
                                logger.info(f"ç”Ÿæˆ {col} åˆ—çš„å…³é”®è¯è¯äº‘")
                            except Exception as wordcloud_e:
                                logger.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥ï¼Œæ˜¾ç¤ºå…³é”®è¯ç»Ÿè®¡æ›¿ä»£: {wordcloud_e}")
                                # æ˜¾ç¤ºå…³é”®è¯ç»Ÿè®¡
                                st.write(f"#### {col} - å…³é”®è¯ç»Ÿè®¡")
                                # æ£€æŸ¥plotlyæ˜¯å¦å¯ç”¨
                                if plotly_available and px is not None:
                                    try:
                                        fig = px.bar(
                                            x=list(top_keywords.keys()),
                                            y=list(top_keywords.values()),
                                            labels={'x': 'å…³é”®è¯', 'y': 'å‡ºç°æ¬¡æ•°'},
                                            title=f"{col} - å…³é”®è¯å‡ºç°æ¬¡æ•°"
                                        )
                                        st.plotly_chart(fig)
                                    except Exception as bar_e:
                                        logger.error(f"æŸ±çŠ¶å›¾ç”Ÿæˆå¤±è´¥ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£: {bar_e}")
                                        st.write("### å…³é”®è¯å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                                        for keyword, count in top_keywords.items():
                                            st.write(f"- **{keyword}**: {count} æ¬¡")
                                else:
                                    logger.info("plotlyä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£")
                                    st.write("### å…³é”®è¯å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                                    for keyword, count in top_keywords.items():
                                        st.write(f"- **{keyword}**: {count} æ¬¡")
                        else:
                            logger.info("matplotlibä¸å¯ç”¨ï¼Œæ˜¾ç¤ºå…³é”®è¯ç»Ÿè®¡æ›¿ä»£")
                            st.write(f"#### {col} - å…³é”®è¯ç»Ÿè®¡")
                            # æ£€æŸ¥plotlyæ˜¯å¦å¯ç”¨
                            if plotly_available and px is not None:
                                try:
                                    fig = px.bar(
                                        x=list(top_keywords.keys()),
                                        y=list(top_keywords.values()),
                                        labels={'x': 'å…³é”®è¯', 'y': 'å‡ºç°æ¬¡æ•°'},
                                        title=f"{col} - å…³é”®è¯å‡ºç°æ¬¡æ•°"
                                    )
                                    st.plotly_chart(fig)
                                except Exception as bar_e:
                                    logger.error(f"æŸ±çŠ¶å›¾ç”Ÿæˆå¤±è´¥ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£: {bar_e}")
                                    st.write("### å…³é”®è¯å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                                    for keyword, count in top_keywords.items():
                                        st.write(f"- **{keyword}**: {count} æ¬¡")
                                else:
                                    logger.info("plotlyä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£")
                                    st.write("### å…³é”®è¯å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                                    for keyword, count in top_keywords.items():
                                        st.write(f"- **{keyword}**: {count} æ¬¡")
                    except Exception as e:
                        st.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                        logger.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                        st.write(f"#### {col} - å…³é”®è¯ç»Ÿè®¡")
                        # æ£€æŸ¥plotlyæ˜¯å¦å¯ç”¨
                        if plotly_available and px is not None:
                            try:
                                fig = px.bar(
                                    x=list(top_keywords.keys()),
                                    y=list(top_keywords.values()),
                                    labels={'x': 'å…³é”®è¯', 'y': 'å‡ºç°æ¬¡æ•°'},
                                    title=f"{col} - å…³é”®è¯å‡ºç°æ¬¡æ•°"
                                )
                                st.plotly_chart(fig)
                            except Exception as bar_e:
                                logger.error(f"æŸ±çŠ¶å›¾ç”Ÿæˆå¤±è´¥ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£: {bar_e}")
                                st.write("### å…³é”®è¯å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                                for keyword, count in top_keywords.items():
                                    st.write(f"- **{keyword}**: {count} æ¬¡")
                            else:
                                logger.info("plotlyä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£")
                                st.write("### å…³é”®è¯å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                                for keyword, count in top_keywords.items():
                                    st.write(f"- **{keyword}**: {count} æ¬¡")
                        logger.info(f"ç”Ÿæˆ {col} åˆ—çš„å…³é”®è¯ç»Ÿè®¡æŸ±çŠ¶å›¾")
    
    elif analysis_type == "æ ‡ç­¾æå–":
        # æ˜¾ç¤ºæ ‡ç­¾æŸ±çŠ¶å›¾
        for col in analyzed_columns:
            result_col = f"{col}_{analysis_type}ç»“æœ"
            if result_col in result_df.columns:
                # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
                all_tags = ", ".join(result_df[result_col].fillna("").astype(str)).split(", ")
                # æ¸…ç†ç©ºç™½æ ‡ç­¾
                all_tags = [tag.strip() for tag in all_tags if tag.strip()]
                
                if all_tags:
                    # ç»Ÿè®¡æ ‡ç­¾å‡ºç°æ¬¡æ•°
                    tag_counts = Counter(all_tags)
                    top_tags = dict(tag_counts.most_common(10))
                    
                    st.write(f"#### {col} - æ ‡ç­¾ç»Ÿè®¡")
                    # æ£€æŸ¥plotlyæ˜¯å¦å¯ç”¨
                    if plotly_available and px is not None:
                        try:
                            fig = px.bar(
                                x=list(top_tags.keys()),
                                y=list(top_tags.values()),
                                labels={'x': 'æ ‡ç­¾', 'y': 'å‡ºç°æ¬¡æ•°'},
                                title=f"{col} - æ ‡ç­¾å‡ºç°æ¬¡æ•°",
                                color_discrete_sequence=px.colors.qualitative.Pastel
                            )
                            st.plotly_chart(fig)
                            logger.info(f"ç”Ÿæˆ {col} åˆ—çš„æ ‡ç­¾ç»Ÿè®¡æŸ±çŠ¶å›¾")
                        except Exception as tag_e:
                            logger.error(f"æ ‡ç­¾æŸ±çŠ¶å›¾ç”Ÿæˆå¤±è´¥ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£: {tag_e}")
                            st.write("### æ ‡ç­¾å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                            for tag, count in top_tags.items():
                                st.write(f"- **{tag}**: {count} æ¬¡")
                    else:
                        logger.info("plotlyä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡æ›¿ä»£")
                        st.write("### æ ‡ç­¾å‡ºç°æ¬¡æ•°ç»Ÿè®¡")
                        for tag, count in top_tags.items():
                            st.write(f"- **{tag}**: {count} æ¬¡")
                        logger.info(f"ç”Ÿæˆ {col} åˆ—çš„æ ‡ç­¾ç»Ÿè®¡æŸ±çŠ¶å›¾")
    
    # ç»“æœå¯¼å‡º
    st.subheader("å¯¼å‡ºç»“æœ")
    
    # å°†DataFrameè½¬æ¢ä¸ºExcel
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        result_df.to_excel(writer, index=False, sheet_name='åˆ†æç»“æœ')
    output.seek(0)
    
    # æä¾›ä¸‹è½½æŒ‰é’®
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½ç»“æœæ–‡ä»¶",
        data=output,
        file_name="æ™ºèƒ½åˆ†æç»“æœ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    logger.info("æä¾›ç»“æœæ–‡ä»¶ä¸‹è½½")
else:
    st.info("è¯·å…ˆå®Œæˆåˆ†æä»¥æŸ¥çœ‹ç»“æœ")

# æ·»åŠ åº”ç”¨è¯´æ˜
st.sidebar.header("ğŸ“˜ ä½¿ç”¨è¯´æ˜")
st.sidebar.markdown("""
1. ä¸Šä¼ Excelæ–‡ä»¶ï¼ˆ.xlsx æˆ– .xlsï¼‰
2. é€‰æ‹©è¦åˆ†æçš„åˆ—
3. é€‰æ‹©åˆ†æç±»å‹
4. ç‚¹å‡»"å¼€å§‹åˆ†æ"
5. æŸ¥çœ‹ç»“æœå’Œå¯è§†åŒ–å›¾è¡¨
6. å¯è¿›è¡Œäººå·¥ä¿®æ­£
7. ä¸‹è½½åˆ†æç»“æœæ–‡ä»¶
""")